[{"link": "https://www.seek.com.au/job/79280652", "description": "Melbourne based (Hybrid working model) - Initial 3 months contract with likely extensions!\nWork as a Data Engineering Manager with a leading Consulting firm on one of their Melbourne based projects.\nResponsibilities and Skills required:\nData Base Architecture & Modelling\nData Vault 2 modelling techniques is a must have\nDbt Data Engineering experience:\nConfig yml files\nSchema creation and tests\nGit Lab pipelines\nGCP Big Query hands on\nSQL\nPython\nTo be considered for the role click the 'apply' button or for more information about this and other opportunities please contact Yash Kumar Jain on 03 86804235 or email: ykumarjain@paxus.com.au and quote the above job reference number.", "title": "Data Engineer Manager: GCP and Data Vault Expertise", "job_id": "79280652", "score": 8, "cover_letter": "\nAs a seasoned Data Consultant with a strong background in data engineering, architecture, and cloud platforms, I am excited to leverage my expertise to drive data engineering excellence as a Data Engineering Manager with your team. With hands-on experience in GCP and proficiency in skills such as SQL, Python, and data modeling, I am confident that I can make a significant impact in this role, bringing innovative data solutions to life."}, {"link": "https://www.seek.com.au/job/79275359", "description": "Design scalable data solutions using AWS, PySpark, and Databricks\nDrive innovation in data architecture with a focus on advanced cloud and big data technologies\nCollaborate with cross-functional teams to deliver high-performance data platforms\n\nCompany Overview\n\nA leader in wealth management sector is driving innovation through data engineering and cloud infrastructure. As part of our team, you will work with AWS, PySpark, and Databricks to develop data solutions that drive key business decisions. Their focus is on leveraging the best in cloud technology and advanced analytics to build scalable, efficient data platforms that deliver real business value.\n  \nRole Overview\n\nWe are seeking a highly skilled Data Engineer/Data Architect with expertise in AWS Cloud (EMR, Athena, Airflow), PySpark, and Databricks. In this role, you will play a critical part in designing and implementing advanced data solutions on AWS and related technologies. As a forward-thinking professional, you will balance delivering technical outcomes with understanding the broader business impact. Your strategic approach will ensure that immediate challenges, future needs, and emerging opportunities are all considered in the solutions you design\n\nKey Responsibilities:\nLogical Data Models & Schema Design: Perform logical data modeling and schema design, leveraging PySpark and Databricks to build physical data models for enhanced data processing and analytics.\nAnalyze Structural Requirements: Evaluate and determine the structural requirements for new software and applications, ensuring alignment with business needs.\nGuidance & Support: Provide guidance and support to application developers, ensuring adherence to data architecture standards and best practices.\nData Migration & ETL: Lead the analysis, design, and execution of data migration and ETL tasks from legacy systems to modern solutions, utilizing PySpark for efficient data processing.\nCollaboration with Data Science: Work closely with the Data Science team to identify and address future data needs, ensuring alignment with business goals and analytics requirements.\nApplication Data Mapping: Support data mapping, SQL query tuning, schema enhancements, and code expansion for application integration and optimization.\nData Visualization: Use data visualization tools to present insights effectively, aiding decision-making across business teams.\nData & Schema Standards: Maintain a strong understanding of data and schema standards, ensuring quality, consistency, and governance across all data activities.\nData Mining & Segmentation: Apply advanced data mining and segmentation techniques using Databricks to extract meaningful insights from large datasets.\nDocumentation: Create and maintain comprehensive documentation for data models, schemas, and data migration processes, ensuring clarity and consistency.\nSkills & Experience\n\nWe are looking for a highly skilled technical professional with:\nExperience: At least 3 years of experience in data architecture or engineering, with a focus on application data modeling, DB schema design, ETL execution, and SQL query tuning. Significant experience with PySpark and Databricks is essential.\nAWS Cloud Expertise: Deep knowledge of AWS services, including EMR, Athena, Airflow, S3, Lambda, and Glue, with experience in integrating cloud-based data solutions.\nPySpark & Databricks Proficiency: Proven experience using PySpark for data processing and Databricks for advanced analytics and large-scale data mining.\nSQL & PL/SQL: Strong expertise in SQL query optimization, schema design, and PL/SQL programming.\nSODA Knowledge: Experience working with Service-Oriented Data Architecture (SODA) to design scalable, service-oriented data solutions.\nETL & Data Pipelines: Expertise in designing automated ETL pipelines for efficient data migration and transformation.\nData Visualization Tools: Experience with tools like Tableau, Power BI, or AWS QuickSight for delivering actionable insights.\nEducation: Degree in Computer Science, Information Systems, or a related technical field.\nWhat\u2019s on Offer\n\nThis is a unique opportunity to join a highly technical, forward-thinking team working on large-scale data projects using AWS, PySpark, and Databricks. We offer a collaborative environment with a strong focus on professional development, innovation, and work-life balance.\n\nKey benefits include:\nCompetitive daily rate\nOngoing contract potential\nOpportunities to work on complex, high-impact data engineering projects.\nFlexible working environment with a focus on work-life balance.\nApply to Dave Marshall (David.marshall@profusiongroup.com) via the link today.\n\n\nProfusion respect people, value diversity and are committed to equality. We are committed to providing a supportive culture and positively contributing towards creating diverse and inclusive workplaces for our candidates & clients. We invite candidates of all ages, genders, sexual orientation, cultural backgrounds, people with disability, neurodiverse individuals, and Indigenous Australians to apply.", "title": "Data Engineer - Investment Data", "job_id": "79275359", "score": 8, "cover_letter": "\nWith my extensive experience in data engineering and cloud infrastructure, I am confident that I can leverage my expertise to drive innovation in data architecture at your organization, particularly with my background in working with AWS, PySpark, and Databricks. I am excited to apply my skills and expertise to design scalable data solutions and drive business growth through advanced cloud technologies."}, {"link": "https://www.seek.com.au/job/79233903", "description": "A Melbourne-based Education organisation is looking for a Senior Data Engineer lead a critical Data Project which will drive the maturity of the Data Practice.\n\nThe individual will be the the Sole Data Engineer, tasked with gathering, cleaning, and amalgamating data from internal and external sources (with support of other Data Specialists).\n\nThe ideal candidate will apply their programming, mathematical, and analytical skills to process structured and unstructured data (including JSON), building scalable cloud data systems in AWS, and validating and analyzing large datasets to identify patterns and solutions.\n\nKey responsibilities include enhancing data collection procedures, designing and maintaining cloud-based data pipelines, and presenting analytical results clearly. You'll also propose data-driven strategies to tackle business challenges and collaborate closely with Business and IT teams.\n\n\nKey Skills Required:\nExpertise in database systems, AI, numerical analysis, HCI, and software engineering.\nStrong knowledge of data architectures, ETL processes, data warehousing, and both relational and non-relational databases.\nProficiency in AWS services (EC2, S3, Redshift, Lambda, Glue) for data processing and deployment.\nExperience with in  Python/Java and SQL.\nAbility to connect with stakeholders to translate business problems into data-driven solutions.\nEffective working with cross-functional teams and communicating technical insights to diverse audiences.\nDaily Rate: $850-1050p/d - initial 6 Month contract + Extensions\nWFH: SE Suburbs - 1 day per Fortnight in office\n\n\nIf you are interested in transforming the way an international organisation uses data then get in touch for more information - Apply here or email Jonny.Church@pra.com.au for more info", "title": "Senior Data Engineer/ Data Lead - 6 Month Contract", "job_id": "79233903", "score": 9, "cover_letter": "\n As a seasoned data professional with a strong background in data engineering, cloud infrastructure, and statistical modelling, I am excited about the opportunity to lead the critical Data Project at your organisation and drive the maturity of the Data Practice. With my expertise in AWS services, Python, and SQL, I am confident in my ability to design and maintain cloud-based data pipelines and provide data-driven solutions to tackle business challenges."}, {"link": "https://www.seek.com.au/job/79297688", "description": "Talent Insights are currently on the lookout for an Actuarial Analyst wanting to gain further experience/exposure to Data Science.\n\nIn this role you will be joining one of Australia's best federal government departments on a 12 - 24 month contract. The role is based in Melbourne or Sydney (NSW) - is Hybrid working.\n\nIn this position you will be working within the Analytics, Data and Actuarial division.\n\nResponsibilities may include but are not limited to: \nExtracting, cleansing and preparing of data from multiple sources\nRegular performance monitoring\nUtilising statistical and mathematical modelling techniques to undertake analytics work on a range of tasks, including actuarial monitoring and analyses, data tabulations, projects and cost benefit analyses \nAdhoc modelling requests and report production\nStakeholder management - internal and external \nTechnical skills: \nUnderstanding of modelling techniques - GLMs, GBMs experience \nExperience in SAS and/or SQL \nData Analytics/Data Science experience \nAdvanced R programming skills \nAcademic qualification in Actuarial studies, Mathematics/Statistics \nWorking towards Associates or Fellowship qualifications \nIf you possess all or some of the required skills and experience, then we encourage you to apply for this exciting opportunity. Our client is committed to providing a flexible and supportive working environment that promotes a positive work-life balance. \n  \nFor more information please email francesca@talentinsights.com.au", "title": "APS5 Actuary", "job_id": "79297688", "score": 8, "cover_letter": "\nAs a seasoned Data Consultant with experience in statistical modeling and data analysis, I am excited to leverage my skills to transition into an Actuarial Analyst role and contribute to the Analytics, Data and Actuarial division. With a strong foundation in data science and a passion for mathematical modeling, I am well-positioned to extract insights and provide data-driven recommendations in this role."}, {"link": "https://www.seek.com.au/job/79229744", "description": "About the Company\nDavidson has proudly partnered with a leading Australian retailer to assist them in sourcing a Project Manager for their data program.\n\nAbout the Role\nSitting within the data function of the organisation, as the project manager your responsibilities will include but not limited to:\nDelivering complex technology initiatives to achieve business objectives.\nManaging project scope, cost, schedule and risks\nWorking with the Data privacy and security team\nManaging the cloud platform (Azure)\nManaging the data engineering, data modelling and architecture\nAbout You\nSignificant expereince managing data centric projects\nExpereince with Azure\nExpereince with data engineering (ETL pipelines, integration) and data modelling\nStrong understanding of data privacy and security\nExpereince managing projects within the retail industry (Desirable)\nThe Benefits\nCompetitive day rates\n6-month initial contract\nHybrid working arrangements\n\n\n\n\nPlease apply by uploading your current resume in Microsoft Word format only (.doc or .docx). If you would like to have a confidential discussion, please contact Maaz Mian on Maaz.Mian@Davidsonwp.com, quoting reference JN -102024-38362. Want to know more about Davidson? Visit us at www.davidsonwp.com", "title": "Project Manager - Data", "job_id": "79229744", "score": 8, "cover_letter": "\nAs a seasoned Data Consultant with a strong background in data science, engineering, and BI development, I'm confident in my ability to deliver complex technology initiatives that drive business objectives, making me an ideal fit for this Project Manager - Data role. My experience in managing cloud platforms, data engineering, and data modelling, combined with my adaptability and passion for data-driven decision-making, make me excited to discuss this opportunity further."}, {"link": "https://www.seek.com.au/job/79274998", "description": "Design advanced data solutions using AWS Cloud (EMR, Athena, Airflow), SODA, and SQL\nBe a technical leader in Investment Data Engineering with a focus on high-performance architectures\nShape the future of data infrastructure with cutting-edge cloud and SQL expertise\n\nCompany Overview\n\nA leader in wealth management sector is driving innovation through data engineering and cloud infrastructure. My client is committed to using the latest technologies, including AWS Cloud and SQL, alongside Service-Oriented Data Architecture (SODA), to develop scalable data platforms.\n  \nRole Overview\n\nAs a Data Architect within Investment Data Engineering, you will design and implement scalable, high-performance data architectures on AWS. You\u2019ll focus on leveraging advanced SQL skills, SODA principles, and AWS cloud services to build robust data solutions that support the company\u2019s business and analytics objectives. You will collaborate with Data Science and DevOps teams to ensure seamless data flow and alignment with business needs.\n\nKey Responsibilities:\nData Infrastructure Design: Architect scalable, service-oriented data solutions using AWS Cloud (EMR, Athena, Airflow) and SODA, optimized for large-scale data processing.\nSQL Expertise: Lead the design and optimization of complex SQL queries, ensuring efficient data retrieval and database performance across distributed systems.\nSODA Implementation: Use SODA principles to design modular, reusable data services that enable seamless integration between applications.\nData Modelling & Schema Design: Develop logical and physical data models with a focus on high-performance schema design and optimization.\nETL & Data Migration: Lead data migration efforts, designing and implementing automated ETL processes using AWS Glue and Lambda to transfer data from legacy systems.\nAutomation & Orchestration: Automate data workflows using Airflow, AWS Step Functions, and advanced SQL techniques to ensure real-time data processing and orchestration.\nData Governance & Quality: Implement best practices in data governance, maintaining data quality and security throughout the architecture.\nQuery Optimization & Tuning: Use advanced SQL query optimization techniques to enhance performance, minimize latency, and improve processing efficiency for large datasets.\nCollaboration with Data Science: Partner with the Data Science team to ensure data architecture aligns with future business goals, including real-time analytics and machine learning initiatives.\nData Visualization: Utilize data visualization tools such as Tableau or QuickSight to present insights and support decision-making.\n  \nSkills & Experience\n\nWe are looking for a highly skilled technical professional with:\nExperience: 3+ years as a Data Architect, Data Engineer, or related role, with deep experience in SQL, data modelling, and cloud-based data solutions.\nAWS Expertise: Hands-on experience with AWS services (EMR, Athena, S3, Lambda, Airflow, Redshift, Glue) to build and maintain scalable data architectures.\nSODA Expertise: Strong understanding and practical application of Service-Oriented Data Architecture (SODA) for designing modular and reusable data services.\nSQL Proficiency: Advanced SQL skills for data modelling, query optimization, database tuning, and ETL automation.\nETL & Data Pipelines: Proven ability to design and implement scalable ETL pipelines, including data migration and transformation processes.\nProgramming Skills: Proficiency in PL/SQL, Python, and other scripting languages for automation and orchestration.\nData Visualization: Experience with data visualization tools such as Tableau, Power BI, or AWS QuickSight for presenting insights.\nEducation: A degree in Computer Science, Information Systems, or a related technical field.\nWhat\u2019s on Offer\n\nJoin a highly technical team at the forefront of investment data engineering, where you\u2019ll work on large-scale data projects utilizing AWS Cloud, SODA, and SQL technologies. Join a collaborative environment that encourages continuous learning and innovation.\n\nKey benefits include:\nCompetitive daily rate\nOngoing contract potential\nOpportunities to work on complex, high-impact data engineering projects.\nFlexible working environment with a focus on work-life balance.\nApply to Dave Marshall (David.marshall@profusiongroup.com) via the link today.\n\n\nProfusion respect people, value diversity and are committed to equality. We are committed to providing a supportive culture and positively contributing towards creating diverse and inclusive workplaces for our candidates & clients. We invite candidates of all ages, genders, sexual orientation, cultural backgrounds, people with disability, neurodiverse individuals, and Indigenous Australians to apply.", "title": "Data Architect - Investment Data", "job_id": "79274998", "score": 9, "cover_letter": "\n As a seasoned data professional with a strong background in data science, data engineering, and cloud infrastructure, I am confident in my ability to design and implement scalable, high-performance data architectures as a Data Architect within Investment Data Engineering. With my experience in leveraging advanced technologies such as AWS Cloud, SQL, and SODA, I am well-positioned to drive innovation and support the company's business and analytics objectives."}]